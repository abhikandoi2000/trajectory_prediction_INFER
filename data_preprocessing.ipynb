{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 31.8 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 11.2 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import PIL\n",
    "import os\n",
    "from skimage import color\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from pyquaternion import Quaternion\n",
    "#nusc = NuScenes(version='v1.0-mini', dataroot='data/sets/nuscenes', verbose=True)\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot='full_data/sets/nuscenes', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def project_points_to_BEV(pc, lidar_data, pose_record):\n",
    "#     cs_record = nusc.get('calibrated_sensor', lidar_data['calibrated_sensor_token'])\n",
    "    \n",
    "#     ref_to_ego = transform_matrix(translation=cs_record['translation'],\n",
    "#                                   rotation=Quaternion(cs_record[\"rotation\"]))\n",
    "\n",
    "#     # Compute rotation between 3D vehicle pose and \"flat\" vehicle pose (parallel to global z plane).\n",
    "#     ego_yaw = Quaternion(pose_record['rotation']).yaw_pitch_roll[0]\n",
    "#     rotation_vehicle_flat_from_vehicle = np.dot(\n",
    "#         Quaternion(scalar=np.cos(ego_yaw / 2), vector=[0, 0, np.sin(ego_yaw / 2)]).rotation_matrix,\n",
    "#         Quaternion(pose_record['rotation']).inverse.rotation_matrix)\n",
    "#     vehicle_flat_from_vehicle = np.eye(4)\n",
    "#     vehicle_flat_from_vehicle[:3, :3] = rotation_vehicle_flat_from_vehicle\n",
    "#     viewpoint = np.dot(vehicle_flat_from_vehicle, ref_to_ego) \n",
    "#     points = view_points(pc.points[:3, :], viewpoint, normalize=False)\n",
    "#     return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {}\n",
    "\n",
    "palette['Bird'] = [165, 42, 42]\n",
    "palette['Ground Animal'] = [0, 192, 0]\n",
    "palette['Curb'] = [196, 196, 196]\n",
    "palette['Fence'] = [190, 153, 153]\n",
    "palette['Guard Rail'] = [180, 165, 180]\n",
    "palette['Barrier'] = [90, 120, 150]\n",
    "palette['Wall'] = [102, 102, 156]\n",
    "palette['Bile Lane'] = [128, 64, 255]\n",
    "palette['Crosswalk - Plain'] = [140, 140, 200]\n",
    "palette['Curb Cut'] = [170, 170, 170]\n",
    "palette['Parking'] = [250, 170, 160]\n",
    "palette['Pedestrian Area'] = [96, 96, 96]\n",
    "palette['Rail Track'] = [230, 150, 140]\n",
    "palette['Road'] = [128, 64, 128]\n",
    "palette['Service Lane'] = [110, 110, 110]\n",
    "palette['Sidewalk'] = [244, 35, 232]\n",
    "palette['Bridge'] = [150, 100, 100]\n",
    "palette['Building'] = [70, 70, 70]\n",
    "palette['Tunnel'] = [150, 120, 90]\n",
    "palette['Person'] = [220, 20, 60]\n",
    "palette['Bicyclist'] = [255, 0, 0]\n",
    "palette['Motorcyclist'] = [255, 0, 100]\n",
    "palette['Other Rider'] = [255, 0, 200]\n",
    "palette['Lane Marking - Crosswalk'] = [200, 128, 128]\n",
    "palette['Lane Marking - General'] = [255, 255, 255]\n",
    "palette['Mountain'] = [64, 170, 64]\n",
    "palette['Sand'] = [230, 160, 50]\n",
    "palette['Sky'] = [70, 130, 180]\n",
    "palette['Snow'] = [190, 255, 255]\n",
    "palette['Terrain'] = [152, 251, 152]\n",
    "palette['Vegetation'] = [107, 142, 35]\n",
    "palette['Water'] = [0, 170, 30]\n",
    "palette['Banner'] = [255, 255, 128]\n",
    "palette['Bench'] = [250, 0, 30]\n",
    "palette['Bike Rack'] = [100, 140, 180]\n",
    "palette['Billboard'] = [220, 220, 220]\n",
    "palette['Catch Basin'] = [220, 128, 128]\n",
    "palette['CCTV Camera'] = [222, 40, 40]\n",
    "palette['Fire Hydrant'] = [100, 170, 30]\n",
    "palette['Junction Box'] = [40, 40, 40]\n",
    "palette['Mailbox'] = [33, 33, 33]\n",
    "palette['Manhole'] = [100, 128, 160]\n",
    "palette['Phone Booth'] = [142, 0, 0]\n",
    "palette['Pothole'] = [70, 100, 150]\n",
    "palette['Street Light'] = [210, 170, 100]\n",
    "palette['Pole'] = [153, 153, 153]\n",
    "palette['Traffic Sign Frame'] = [128, 128, 128]\n",
    "palette['Utility Pole'] = [0, 0, 80]\n",
    "palette['Traffic Light'] = [250, 170, 30]\n",
    "palette['Traffic Sign (Back)'] = [192, 192, 192]\n",
    "palette['Traffic Sign (Front)'] = [220, 220, 0]\n",
    "palette['Trash Can'] = [140, 140, 20]\n",
    "palette['Bicycle'] = [119, 11, 32]\n",
    "palette['Boat'] = [150, 0, 255]\n",
    "palette['Bus'] = [0, 60, 100]\n",
    "palette['Car'] = [0, 0, 142]\n",
    "palette['Caravan'] = [0, 0, 90]\n",
    "palette['Motorcycle'] = [0, 0, 230]\n",
    "palette['On Rails'] = [0, 80, 100]\n",
    "palette['Other Vehicle'] = [128, 64, 64]\n",
    "palette['Trailer'] = [0, 0, 110]\n",
    "palette['Truck'] = [0, 0, 70]\n",
    "palette['Wheeled Slow'] = [0, 0, 192]\n",
    "palette['Car Mount'] = [32, 32, 32]\n",
    "palette['Ego Vehicle'] = [120, 10, 10]\n",
    "\n",
    "for k in palette.keys():\n",
    "    palette[k].append(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_to_classidx = {}\n",
    "class_to_idx = {}\n",
    "count = 0 \n",
    "for k in palette: \n",
    "    pixel_to_classidx[tuple(palette[k])] = (k, count)\n",
    "    class_to_idx[k] = count\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Bird',\n",
       " 1: 'Ground Animal',\n",
       " 2: 'Curb',\n",
       " 3: 'Fence',\n",
       " 4: 'Guard Rail',\n",
       " 5: 'Barrier',\n",
       " 6: 'Wall',\n",
       " 7: 'Bile Lane',\n",
       " 8: 'Crosswalk - Plain',\n",
       " 9: 'Curb Cut',\n",
       " 10: 'Parking',\n",
       " 11: 'Pedestrian Area',\n",
       " 12: 'Rail Track',\n",
       " 13: 'Road',\n",
       " 14: 'Service Lane',\n",
       " 15: 'Sidewalk',\n",
       " 16: 'Bridge',\n",
       " 17: 'Building',\n",
       " 18: 'Tunnel',\n",
       " 19: 'Person',\n",
       " 20: 'Bicyclist',\n",
       " 21: 'Motorcyclist',\n",
       " 22: 'Other Rider',\n",
       " 23: 'Lane Marking - Crosswalk',\n",
       " 24: 'Lane Marking - General',\n",
       " 25: 'Mountain',\n",
       " 26: 'Sand',\n",
       " 27: 'Sky',\n",
       " 28: 'Snow',\n",
       " 29: 'Terrain',\n",
       " 30: 'Vegetation',\n",
       " 31: 'Water',\n",
       " 32: 'Banner',\n",
       " 33: 'Bench',\n",
       " 34: 'Bike Rack',\n",
       " 35: 'Billboard',\n",
       " 36: 'Catch Basin',\n",
       " 37: 'CCTV Camera',\n",
       " 38: 'Fire Hydrant',\n",
       " 39: 'Junction Box',\n",
       " 40: 'Mailbox',\n",
       " 41: 'Manhole',\n",
       " 42: 'Phone Booth',\n",
       " 43: 'Pothole',\n",
       " 44: 'Street Light',\n",
       " 45: 'Pole',\n",
       " 46: 'Traffic Sign Frame',\n",
       " 47: 'Utility Pole',\n",
       " 48: 'Traffic Light',\n",
       " 49: 'Traffic Sign (Back)',\n",
       " 50: 'Traffic Sign (Front)',\n",
       " 51: 'Trash Can',\n",
       " 52: 'Bicycle',\n",
       " 53: 'Boat',\n",
       " 54: 'Bus',\n",
       " 55: 'Car',\n",
       " 56: 'Caravan',\n",
       " 57: 'Motorcycle',\n",
       " 58: 'On Rails',\n",
       " 59: 'Other Vehicle',\n",
       " 60: 'Trailer',\n",
       " 61: 'Truck',\n",
       " 62: 'Wheeled Slow',\n",
       " 63: 'Car Mount',\n",
       " 64: 'Ego Vehicle'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx\n",
    "idx_to_class = {class_to_idx[k] : k for k in class_to_idx}\n",
    "idx_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seg(og_seg, plot_images=False, colortype='rgb'):\n",
    "    '''\n",
    "    og_seg - image from which intermediate representations will be extracted from (array)\n",
    "    plot_images - plots images if True, does not plot if False\n",
    "    colortype - 'binary': plots 1 or 0 (used for occupancy grid), 'grayscale': plots in grayscale for lidar mapping,\n",
    "                'rgb' or any other string: plots in original palette colors\n",
    "    returns ret - list of intermediate representations(road, lane, and obstacle in that order so far)\n",
    "    '''\n",
    "    if(plot_images):\n",
    "        plt.figure()\n",
    "        plt.imshow(og_seg)\n",
    "    ret = []\n",
    "    #road segmentation\n",
    "    inter_seg_road = copy.deepcopy(og_seg)\n",
    "    inter_seg_road[(og_seg != palette['Road']).any(axis=2)] = [0,0,0,255]  \n",
    "    ret.append(inter_seg_road)\n",
    "    \n",
    "    #lane segmentation\n",
    "    inter_seg_lane = copy.deepcopy(og_seg)\n",
    "    crosswalk = (og_seg != palette['Lane Marking - Crosswalk']).any(axis=2)\n",
    "    general = (og_seg != palette['Lane Marking - General']).any(axis=2) \n",
    "    inter_seg_lane[np.logical_and(crosswalk, general)] = [0,0,0,255] \n",
    "    ret.append(inter_seg_lane)\n",
    "    \n",
    "    #obstacle segmentation (did not include Curb Cut as obstacle but did include Curb)\n",
    "    inter_seg_obstacle = copy.deepcopy(og_seg)\n",
    "    building = (og_seg != palette['Building']).any(axis=2)\n",
    "    curb = (og_seg != palette['Curb']).any(axis=2)\n",
    "    vegetation = (og_seg != palette['Vegetation']).any(axis=2)\n",
    "    inter_seg_obstacle[np.logical_and(np.logical_and(building,curb),vegetation)] = [0,0,0,255]\n",
    "    ret.append(inter_seg_obstacle)\n",
    "    \n",
    "    if(colortype == 'grayscale'):\n",
    "        for i in range(0, len(ret)):\n",
    "            temp = color.rgb2gray(ret[i])\n",
    "            ret[i] = temp\n",
    "            \n",
    "    elif(colortype == 'binary'):\n",
    "        for i in range(0, len(ret)):\n",
    "            temp = color.rgb2gray(ret[i])\n",
    "            temp[temp > 0] = 1\n",
    "            ret[i] = temp\n",
    "    \n",
    "    \n",
    "    if(plot_images):\n",
    "        for i in ret:\n",
    "            plt.figure()\n",
    "            if(colortype =='grayscale'):\n",
    "                plt.imshow(i, cmap='gray')\n",
    "            else:\n",
    "                plt.imshow(i)\n",
    "    return ret\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_class(points, cam_img):\n",
    "    num_points = points.shape[1]\n",
    "    \n",
    "    class_vec = [-1] * num_points\n",
    "    num_points_classified = 0\n",
    "    for i in range(num_points):\n",
    "        current_point = np.round(points[:,i][:2])\n",
    "        r = int(current_point[0])\n",
    "        c = int(current_point[1])\n",
    "        object_type, class_id = pixel_to_classidx[(tuple(cam_img[c,r]))]\n",
    "        class_vec[i] = class_id\n",
    "        num_points_classified+=1\n",
    "#     print(str(num_points_classified) + '/' + str(len(class_vec)))\n",
    "            \n",
    "    return class_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points_to_image(current_pc, pointsensor, cam):\n",
    "    \n",
    "    pc = copy.deepcopy(current_pc)\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    # Second step: transform to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "    \n",
    "    t2 = transform_matrix(translation=poserecord['translation'],rotation=Quaternion(poserecord['rotation']),inverse=True)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    global_frame_pc = copy.deepcopy(pc)\n",
    "\n",
    "    # Third step: transform into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "\n",
    "    # Fourth step: transform into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    return pc, global_frame_pc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_img_points(points, depths, im, min_dist=1.0):\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths > min_dist)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'full_data/sets/nuscenes/samples/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603547590.pcd.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dc92e84c1ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcam_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnusc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_token_BACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0morig_pc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLidarPointCloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file_multisweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnusc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LIDAR_TOP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LIDAR_TOP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsweeps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#get ego vehicle pose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nuscenes/utils/data_classes.py\u001b[0m in \u001b[0;36mfrom_file_multisweep\u001b[0;34m(cls, nusc, sample_rec, chan, ref_chan, nsweeps, min_distance)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsweeps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# Load up the pointcloud and remove points close to the sensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mcurrent_pc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnusc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_sd_rec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mcurrent_pc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nuscenes/utils/data_classes.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, file_name)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unsupported filetype {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbr_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'full_data/sets/nuscenes/samples/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603547590.pcd.bin'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "my_scene_token = nusc.field2token('scene', 'name', 'scene-0103')[0]\n",
    "scene_rec = nusc.get('scene', my_scene_token)\n",
    "\n",
    "current_token = scene_rec['first_sample_token']\n",
    "has_more = True \n",
    "scene_rec\n",
    "count = 0 \n",
    "history = []\n",
    "plot_result = True\n",
    "for i in range(scene_rec['nbr_samples']): \n",
    "    \n",
    "    #get current sample data\n",
    "    sample_rec = nusc.get('sample', current_token)\n",
    "\n",
    "    #get camera tokens\n",
    "    camera_token = sample_rec['data']['CAM_FRONT']\n",
    "    camera_token_FRONT_LEFT = sample_rec['data']['CAM_FRONT_LEFT']\n",
    "    camera_token_FRONT_RIGHT = sample_rec['data']['CAM_FRONT_RIGHT']\n",
    "    camera_token_BACK = sample_rec['data']['CAM_BACK']\n",
    "\n",
    "    #get lidar data info\n",
    "    pointsensor_token = sample_rec['data']['LIDAR_TOP']\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "\n",
    "    #get camera info\n",
    "    cam = nusc.get('sample_data', camera_token)\n",
    "    cam_front_left = nusc.get('sample_data', camera_token_FRONT_LEFT)\n",
    "    cam_front_right = nusc.get('sample_data', camera_token_FRONT_RIGHT)\n",
    "    cam_back = nusc.get('sample_data', camera_token_BACK)\n",
    "    \n",
    "    orig_pc, times = LidarPointCloud.from_file_multisweep(nusc, sample_rec, 'LIDAR_TOP', 'LIDAR_TOP', nsweeps=1)\n",
    "    \n",
    "    #get ego vehicle pose\n",
    "    pose_record = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    \n",
    "    #get point cloud in camera frame prior to putting inside image plane\n",
    "    pc, global_frame_pc = project_points_to_image(orig_pc, pointsensor, cam) \n",
    "    pc_front_left, _ = project_points_to_image(orig_pc, pointsensor, cam_front_left)\n",
    "    pc_front_right, _ = project_points_to_image(orig_pc, pointsensor, cam_front_right)\n",
    "    pc_back, _ = project_points_to_image(orig_pc, pointsensor, cam_back)\n",
    "  \n",
    "    #get image representation of camera data\n",
    "    im = PIL.Image.open(osp.join(nusc.dataroot, cam['filename']))\n",
    "    im_fl = PIL.Image.open(osp.join(nusc.dataroot, cam_front_left['filename']))\n",
    "    im_fr = PIL.Image.open(osp.join(nusc.dataroot, cam_front_right['filename']))\n",
    "    im_b = PIL.Image.open(osp.join(nusc.dataroot, cam_back['filename']))\n",
    "\n",
    "     # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = pc.points[2, :]\n",
    "    depths_front_left = pc_front_left.points[2,:]\n",
    "    depths_front_right = pc_front_right.points[2,:]\n",
    "    depths_back = pc_back.points[2,:]\n",
    "    \n",
    "    print(osp.join(nusc.dataroot, cam['filename']))\n",
    "    print(osp.join(nusc.dataroot, cam_front_left['filename']))\n",
    "    print(osp.join(nusc.dataroot, cam_front_right['filename']))\n",
    "    print(osp.join(nusc.dataroot, cam_back['filename']))\n",
    "\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    cs_record_front_left = nusc.get('calibrated_sensor', cam_front_left['calibrated_sensor_token'])\n",
    "    cs_record_front_right = nusc.get('calibrated_sensor', cam_front_right['calibrated_sensor_token'])\n",
    "    cs_record_back = nusc.get('calibrated_sensor', cam_back['calibrated_sensor_token'])\n",
    "    \n",
    "    #get matrix representation of camera image\n",
    "    cam_data_arr_front = plt.imread(osp.join(nusc.dataroot, cam['filename']))\n",
    "    cam_data_arr_front_left = plt.imread(osp.join(nusc.dataroot, cam_front_left['filename']))\n",
    "    cam_data_arr_front_right = plt.imread(osp.join(nusc.dataroot, cam_front_right['filename']))\n",
    "    cam_data_arr_back = plt.imread(osp.join(nusc.dataroot, cam_back['filename']))\n",
    "    \n",
    "    #get point cloud data in the image plane across all cameras\n",
    "    points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)\n",
    "    points_front_left = view_points(pc_front_left.points[:3, :], np.array(cs_record_front_left['camera_intrinsic']), normalize=True)\n",
    "    points_front_right = view_points(pc_front_right.points[:3, :], np.array(cs_record_front_right['camera_intrinsic']), normalize=True)\n",
    "    points_back = view_points(pc_back.points[:3, :], np.array(cs_record_back['camera_intrinsic']), normalize=True)\n",
    "\n",
    "    #get points that are actually inside the image plane\n",
    "    mask_front = mask_img_points(points, depths, im)\n",
    "    mask_front_left = mask_img_points(points_front_left, depths_front_left, im_fl)\n",
    "    mask_front_right = mask_img_points(points_front_right, depths_front_right, im_fr)\n",
    "    mask_back = mask_img_points(points_back, depths_back, im_b)\n",
    "    \n",
    "    # get points inside the image\n",
    "    valid_img_points_front = points[:, mask_front]\n",
    "    valid_img_points_front_left = points_front_left[:, mask_front_left]\n",
    "    valid_img_points_front_right = points_front_right[:, mask_front_right]\n",
    "    valid_img_points_back = points_back[:, mask_back]\n",
    "    \n",
    "    #perform point painting and get class per associated valid lidar point\n",
    "    semantic_class_points_front = get_semantic_class(valid_img_points_front, cam_data_arr_front)   \n",
    "    \n",
    "    semantic_class_points_front_left = get_semantic_class(valid_img_points_front_left, cam_data_arr_front_left)   \n",
    "\n",
    "    semantic_class_points_front_right = get_semantic_class(valid_img_points_front_right, cam_data_arr_front_right)   \n",
    "    semantic_class_points_back = get_semantic_class(valid_img_points_back, cam_data_arr_back)   \n",
    "    \n",
    "    masks = [mask_front, mask_front_left, mask_front_right, mask_back]\n",
    "    semantic_class_points = [semantic_class_points_front,\\\n",
    "                             semantic_class_points_front_left,\\\n",
    "                             semantic_class_points_front_right,\\\n",
    "                             semantic_class_points_back]\n",
    "    \n",
    "    \n",
    "    history.append((pc, global_frame_pc, pointsensor, pose_record, masks, semantic_class_points))\n",
    "    #can take history and predict\n",
    "    if i >= 2*past_seconds and i <= scene_rec['nbr_samples'] - 2*future_seconds: \n",
    "        needed_history = history[-(2*past_seconds+1):-1]\n",
    "        needed_ego_pose = history[-1][3]\n",
    "        \n",
    "        res = 0.75\n",
    "        offset = 64\n",
    "        \n",
    "        \n",
    "        yaw = Quaternion(needed_ego_pose['rotation']).yaw_pitch_roll[0]\n",
    "        rotation_matrix = np.array([[np.cos(yaw), -np.sin(yaw)],[np.sin(yaw), np.cos(yaw)]]).T\n",
    "                \n",
    "        count = i - 2*past_seconds\n",
    "        #with ego pose, transform the corresponding point clouds in history to bev \n",
    "        for pc_h, gpc_h, ps_h, ep_h, m, s_c_p in needed_history:\n",
    "            #generate target rep\n",
    "            shifted_ep_h = np.expand_dims(np.array(ep_h['translation']),1) - np.expand_dims(needed_ego_pose['translation'], 1)\n",
    "            transformed_ep_h = np.dot(rotation_matrix, shifted_ep_h[:2,:])\n",
    "            x = math.floor((int(transformed_ep_h[0]) + offset)/res)\n",
    "            y = math.floor((int(transformed_ep_h[1]) + offset)/res)\n",
    "            target_rep = np.zeros((256,256))\n",
    "            target_rep[y][x] += 1\n",
    "            \n",
    "            \n",
    "            #generate all reps except target\n",
    "            \n",
    "            gpc_h.points[:3,:] -= np.expand_dims(needed_ego_pose['translation'], 1)\n",
    "            gpc_h.points[:2,:] = np.dot(rotation_matrix, gpc_h.points[:2,:])\n",
    "\n",
    "            #ensures we don't get lidar points that are too far away\n",
    "            point_filter_1_x = gpc_h.points[0,:] >= -50\n",
    "            point_filter_1_y = gpc_h.points[1,:] >= -50\n",
    "            point_filter_1 = np.logical_and(point_filter_1_x, point_filter_1_y)\n",
    "            \n",
    "            point_filter_2_x = gpc_h.points[0,:] <= 50\n",
    "            point_filter_2_y = gpc_h.points[1,:] <= 50\n",
    "            point_filter_2 = np.logical_and(point_filter_2_x, point_filter_2_y)\n",
    "\n",
    "            point_filter = np.logical_and(point_filter_1, point_filter_2)\n",
    "        \n",
    "            m_f, m_f_l, m_f_r, m_b = m[0], m[1], m[2], m[3]\n",
    "            s_c_p_f, s_c_p_f_l, s_c_p_f_r, s_c_p_b = s_c_p[0], s_c_p[1], s_c_p[2], s_c_p[3]\n",
    "            overall_mask  = np.logical_or(m_f, m_f_l)\n",
    "            overall_mask  = np.logical_or(overall_mask, m_f_r)            \n",
    "            overall_mask  = np.logical_or(overall_mask, m_b)\n",
    "            \n",
    "            \n",
    "            all_colored = np.array([-1] * gpc_h.points.shape[1])\n",
    "            all_colored[m_f] = s_c_p[0]\n",
    "            all_colored[m_f_l] =  s_c_p[1]\n",
    "            all_colored[m_f_r] = s_c_p[2]\n",
    "            all_colored[m_b] = s_c_p[3]\n",
    "            \n",
    "            m_fl, m_fr = np.logical_and(point_filter, m_f_l),np.logical_and(point_filter, m_f_r)\n",
    "            m_front, m_back = np.logical_and(point_filter, m_f),np.logical_and(point_filter, m_b)\n",
    "            \n",
    "            s_c_p_f = all_colored[m_front]\n",
    "            s_c_p_f_l = all_colored[m_fl]\n",
    "            s_c_p_f_r = all_colored[m_fr]\n",
    "            s_c_p_b = all_colored[m_back]\n",
    "        \n",
    "            road_rep_fl, lane_rep_fl, obstacle_rep_fl, vehicle_rep_fl = get_intermediate_rep(gpc_h.points, m_fl, s_c_p_f_l)\n",
    "            road_rep_fr, lane_rep_fr, obstacle_rep_fr, vehicle_rep_fr = get_intermediate_rep(gpc_h.points, m_fr, s_c_p_f_r)\n",
    "            \n",
    "            road_rep_f, lane_rep_f, obstacle_rep_f, vehicle_rep_f = get_intermediate_rep(gpc_h.points, m_front, s_c_p_f)\n",
    "            road_rep_b, lane_rep_b, obstacle_rep_b, vehicle_rep_b = get_intermediate_rep(gpc_h.points, m_back, s_c_p_b)\n",
    "            \n",
    "            road_rep = road_rep_fl + road_rep_fr + road_rep_f + road_rep_b\n",
    "            lane_rep = lane_rep_fl + lane_rep_fr + lane_rep_f + lane_rep_b\n",
    "            \n",
    "            obstacle_rep = obstacle_rep_fl + obstacle_rep_fr + obstacle_rep_f + obstacle_rep_b\n",
    "            vehicle_rep = vehicle_rep_fl + vehicle_rep_fr + vehicle_rep_f + vehicle_rep_b\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             if  plot_result: \n",
    "#                 #back\n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(gpc_h.points[:,m_b][0,:], gpc_h.points[:,m_b][1,:], c=s_c_p_b, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_back.png\".format(count))\n",
    "#                 plt.clf()\n",
    "                \n",
    "#                 #front \n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(gpc_h.points[:,m_f][0,:], gpc_h.points[:,m_f][1,:],c=s_c_p_f, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_front.png\".format(count))\n",
    "#                 plt.clf()\n",
    "                \n",
    "                \n",
    "#                 #front_left \n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(gpc_h.points[:,m_f_l][0,:], gpc_h.points[:,m_f_l][1,:],c=s_c_p_f_l, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_front_left.png\".format(count))\n",
    "#                 plt.clf()\n",
    "                \n",
    "                \n",
    "#                 #front_right \n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(gpc_h.points[:,m_f_r][0,:], gpc_h.points[:,m_f_r][1,:],c=s_c_p_f_r, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_front_right.png\".format(count))\n",
    "#                 plt.clf()\n",
    "                \n",
    "                \n",
    "#                 #combined\n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(gpc_h.points[:,overall_mask][0,:],gpc_h.points[:,overall_mask][1,:], c = all_colored[all_colored != -1], s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_all.png\".format(count))\n",
    "#                 plt.clf()\n",
    "\n",
    "                \n",
    "                \n",
    "#             count+=1\n",
    "        global_frame_pc.points[:3,:] = global_frame_pc.points[:3,:] - np.expand_dims(needed_ego_pose['translation'], 1)\n",
    "        global_frame_pc.points[:2,:] = np.dot(rotation_matrix, global_frame_pc.points[:2,:])\n",
    "        \n",
    "#         if plot_result: \n",
    "#                 #back\n",
    "#                 import pdb; pdb.set_trace()\n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(global_frame_pc.points[:,mask_back][0,:], global_frame_pc.points[:,mask_back][1,:], c=semantic_class_points_back, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_back.png\".format(count))\n",
    "#                 plt.clf()\n",
    "                \n",
    "#                 #front \n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(global_frame_pc.points[:,mask_front][0,:], global_frame_pc.points[:,mask_front][1,:],c=semantic_class_points_front, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_front.png\".format(count))\n",
    "#                 plt.clf()\n",
    "                \n",
    "                \n",
    "#                 #front_left \n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(global_frame_pc.points[:,mask_front_left][0,:], global_frame_pc.points[:,mask_front_left][1,:],c=semantic_class_points_front_left, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_front_left.png\".format(count))\n",
    "#                 plt.clf()\n",
    "                \n",
    "                \n",
    "#                 #front_right \n",
    "#                 plt.axis((-100,100,-100,100))\n",
    "#                 plt.xlabel('x coordinate w.r.t to current ego pose')\n",
    "#                 plt.ylabel('y coordinate w.r.t to current ego pose')\n",
    "#                 plt.title(\"vehicle pose w.r.t to current pose\")\n",
    "#                 plt.scatter(global_frame_pc.points[:,mask_front_right][0,:], global_frame_pc.points[:,mask_front_right][1,:],c=semantic_class_points_front_right, s = 0.2)\n",
    "#                 plt.savefig(\"painted_history_{}_front_right.png\".format(count))\n",
    "#                 plt.clf()\n",
    "#                 break\n",
    "        \n",
    "    if not sample_rec['next'] == \"\":\n",
    "        current_token = sample_rec['next']\n",
    "    else:\n",
    "        has_more = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(s_c_p):\n",
    "    \n",
    "    objects_found_front = {}\n",
    "\n",
    "    for point in s_c_p[0]:\n",
    "        if idx_to_class[point] not in objects_found_front:\n",
    "            objects_found_front[idx_to_class[point]] = 1\n",
    "        else:\n",
    "             objects_found_front[idx_to_class[point]] += 1\n",
    "\n",
    "    objects_found_front_left = {}\n",
    "\n",
    "    for point in s_c_p[1]:\n",
    "        if idx_to_class[point] not in objects_found_front_left:\n",
    "            objects_found_front_left[idx_to_class[point]] = 1\n",
    "        else:\n",
    "             objects_found_front_left[idx_to_class[point]] += 1\n",
    "\n",
    "    objects_found_front_right = {} \n",
    "\n",
    "    for point in s_c_p[2]:\n",
    "        if idx_to_class[point] not in objects_found_front_right:\n",
    "            objects_found_front_right[idx_to_class[point]] = 1\n",
    "        else:\n",
    "             objects_found_front_right[idx_to_class[point]] += 1\n",
    "\n",
    "    objects_found_back = {} \n",
    "\n",
    "    for point in s_c_p[3]:\n",
    "        if idx_to_class[point] not in objects_found_back:\n",
    "            objects_found_back[idx_to_class[point]] = 1\n",
    "        else:\n",
    "             objects_found_back[idx_to_class[point]] += 1\n",
    "                \n",
    "    print(\" Found in front camera\")\n",
    "    \n",
    "    for class_point in objects_found_front: \n",
    "        print(\"{} found {} times in front camera\".format(class_point, objects_found_front[class_point]))\n",
    "        \n",
    "    print(\" Found in front left camera\")\n",
    "    \n",
    "    for class_point in objects_found_front_left: \n",
    "        print(\"{} found {} times in front camera\".format(class_point, objects_found_front_left[class_point]))  \n",
    "    \n",
    "    print(\" Found in front right camera\")\n",
    "    \n",
    "    for class_point in objects_found_front_right: \n",
    "        print(\"{} found {} times in front camera\".format(class_point, objects_found_front_right[class_point]))  \n",
    "\n",
    "    print(\" Found in back camera\")\n",
    "    \n",
    "    for class_point in objects_found_back: \n",
    "        print(\"{} found {} times in front camera\".format(class_point, objects_found_back[class_point]))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "134 114\n",
      "-63 -63 63 63\n",
      "383 36 732 78 1\n",
      "130 118\n",
      "-63 -63 63 63\n",
      "646 123 1213 173 1\n",
      "128 120\n",
      "-63 -63 63 63\n",
      "592 94 1286 186 1\n",
      "128 124\n",
      "-63 -63 63 63\n",
      "743 101 1344 206 1\n",
      "> <ipython-input-21-8c67e70050d5>(211)<module>()\n",
      "-> base_file_path = \"./representations\"\n",
      "(Pdb) count\n",
      "3\n",
      "(Pdb) x\n",
      "124\n",
      "(Pdb) y\n",
      "128\n",
      "(Pdb) target_rep[y][x]\n",
      "1.0\n",
      "(Pdb) plt.imshow(target_rep)\n",
      "<matplotlib.image.AxesImage object at 0x7f41185d8940>\n",
      "(Pdb) plt.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8c67e70050d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mbase_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./representations\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0;31m#plot the dilated versions of the intermediate reps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-8c67e70050d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mbase_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./representations\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0;31m#plot the dilated versions of the intermediate reps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "past_seconds = 2\n",
    "future_seconds = 4\n",
    "\n",
    "for j in range(len(nusc.scene)//10):\n",
    "    print(j)\n",
    "    my_scene_token = nusc.scene[j]['token']\n",
    "    scene_rec = nusc.get('scene', my_scene_token)\n",
    "    #print(len(scene_rec))\n",
    "\n",
    "    current_token = scene_rec['first_sample_token']\n",
    "    current_seq = []\n",
    "    \n",
    "    for i in range(scene_rec['nbr_samples']): \n",
    "        #get current sample data\n",
    "        sample_rec = nusc.get('sample', current_token)\n",
    "\n",
    "        #get camera tokens\n",
    "        camera_token = sample_rec['data']['CAM_FRONT']\n",
    "        camera_token_FRONT_LEFT = sample_rec['data']['CAM_FRONT_LEFT']\n",
    "        camera_token_FRONT_RIGHT = sample_rec['data']['CAM_FRONT_RIGHT']\n",
    "        camera_token_BACK = sample_rec['data']['CAM_BACK']\n",
    "\n",
    "        pointsensor_token = sample_rec['data']['LIDAR_TOP']\n",
    "        pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "\n",
    "        #get camera info\n",
    "        cam = nusc.get('sample_data', camera_token)\n",
    "        cam_front_left = nusc.get('sample_data', camera_token_FRONT_LEFT)\n",
    "        cam_front_right = nusc.get('sample_data', camera_token_FRONT_RIGHT)\n",
    "        cam_back = nusc.get('sample_data', camera_token_BACK)\n",
    "\n",
    "        orig_pc, times = LidarPointCloud.from_file_multisweep(nusc, sample_rec, 'LIDAR_TOP', 'LIDAR_TOP', nsweeps=10)\n",
    "\n",
    "        #get ego vehicle pose\n",
    "        pose_record = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "\n",
    "        #get point cloud in camera frame prior to putting inside image plane\n",
    "        pc, global_frame_pc = project_points_to_image(orig_pc, pointsensor, cam) \n",
    "        pc_front_left, _ = project_points_to_image(orig_pc, pointsensor, cam_front_left)\n",
    "        pc_front_right, _ = project_points_to_image(orig_pc, pointsensor, cam_front_right)\n",
    "        pc_back, _ = project_points_to_image(orig_pc, pointsensor, cam_back)\n",
    "        \n",
    "#         if 'sweeps' in cam['filename'] or 'sweeps' in cam_front_left['filename'] or 'sweeps' in cam_front_right['filename'] or 'sweeps' in cam_back['filename']:\n",
    "#             print(\"found in sweeps\")\n",
    "\n",
    "        #get image representation of camera data\n",
    "        im = PIL.Image.open(osp.join(nusc.dataroot, cam['filename']))\n",
    "        im_fl = PIL.Image.open(osp.join(nusc.dataroot, cam_front_left['filename']))\n",
    "        im_fr = PIL.Image.open(osp.join(nusc.dataroot, cam_front_right['filename']))\n",
    "        im_b = PIL.Image.open(osp.join(nusc.dataroot, cam_back['filename']))\n",
    "        \n",
    "         # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pc.points[2, :]\n",
    "        depths_front_left = pc_front_left.points[2,:]\n",
    "        depths_front_right = pc_front_right.points[2,:]\n",
    "        depths_back = pc_back.points[2,:]\n",
    "\n",
    "        # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "        cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "        cs_record_front_left = nusc.get('calibrated_sensor', cam_front_left['calibrated_sensor_token'])\n",
    "        cs_record_front_right = nusc.get('calibrated_sensor', cam_front_right['calibrated_sensor_token'])\n",
    "        cs_record_back = nusc.get('calibrated_sensor', cam_back['calibrated_sensor_token'])\n",
    "\n",
    "        #get matrix representation of camera image\n",
    "        cam_data_arr_front = plt.imread(osp.join(nusc.dataroot, cam['filename']))\n",
    "        cam_data_arr_front_left = plt.imread(osp.join(nusc.dataroot, cam_front_left['filename']))\n",
    "        cam_data_arr_front_right = plt.imread(osp.join(nusc.dataroot, cam_front_right['filename']))\n",
    "        cam_data_arr_back = plt.imread(osp.join(nusc.dataroot, cam_back['filename']))\n",
    "\n",
    "        #get point cloud data in the image plane across all cameras\n",
    "        points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)\n",
    "        points_front_left = view_points(pc_front_left.points[:3, :], np.array(cs_record_front_left['camera_intrinsic']), normalize=True)\n",
    "        points_front_right = view_points(pc_front_right.points[:3, :], np.array(cs_record_front_right['camera_intrinsic']), normalize=True)\n",
    "        points_back = view_points(pc_back.points[:3, :], np.array(cs_record_back['camera_intrinsic']), normalize=True)\n",
    "\n",
    "        #get points that are actually inside the image plane\n",
    "        mask_front = mask_img_points(points, depths, im)\n",
    "        mask_front_left = mask_img_points(points_front_left, depths_front_left, im_fl)\n",
    "        mask_front_right = mask_img_points(points_front_right, depths_front_right, im_fr)\n",
    "        mask_back = mask_img_points(points_back, depths_back, im_b)\n",
    "\n",
    "        # get points inside the image\n",
    "        valid_img_points_front = points[:, mask_front]\n",
    "        valid_img_points_front_left = points_front_left[:, mask_front_left]\n",
    "        valid_img_points_front_right = points_front_right[:, mask_front_right]\n",
    "        valid_img_points_back = points_back[:, mask_back]\n",
    "\n",
    "#         import pdb; pdb.set_trace()\n",
    "        \n",
    "        #perform point painting and get class per associated valid lidar point\n",
    "        semantic_class_points_front = get_semantic_class(valid_img_points_front, cam_data_arr_front)   \n",
    "\n",
    "        semantic_class_points_front_left = get_semantic_class(valid_img_points_front_left, cam_data_arr_front_left)   \n",
    "\n",
    "        semantic_class_points_front_right = get_semantic_class(valid_img_points_front_right, cam_data_arr_front_right)   \n",
    "        semantic_class_points_back = get_semantic_class(valid_img_points_back, cam_data_arr_back)   \n",
    "\n",
    "        masks = [mask_front, mask_front_left, mask_front_right, mask_back]\n",
    "        semantic_class_points = [semantic_class_points_front,\\\n",
    "                                 semantic_class_points_front_left,\\\n",
    "                                 semantic_class_points_front_right,\\\n",
    "                                 semantic_class_points_back]\n",
    "        images = [im, im_fl, im_fr, im_b]\n",
    "\n",
    "        current_seq.append((pc, global_frame_pc, pointsensor, pose_record, masks, semantic_class_points, images))\n",
    "        \n",
    "        #create sequences\n",
    "\n",
    "        \n",
    "        if len(current_seq) >= 12:\n",
    "            intermediate_reps = []\n",
    "            \n",
    "            current_frame = i - 2*future_seconds + 1\n",
    "            \n",
    "            time_frame = current_seq[current_frame-2*past_seconds:-1]\n",
    "            needed_ego_pose = current_seq[current_frame][3]\n",
    "\n",
    "            res = 0.5\n",
    "            offset = 64\n",
    "            if len(current_seq) >= 13:\n",
    "                import pdb; pdb.set_trace()\n",
    "\n",
    "            yaw = Quaternion(needed_ego_pose['rotation']).yaw_pitch_roll[0]\n",
    "            rotation_matrix = np.array([[np.cos(yaw), -np.sin(yaw)],[np.sin(yaw), np.cos(yaw)]]).T\n",
    "#             import pdb; pdb.set_trace()\n",
    "            #with ego pose, transform the corresponding point clouds in history to bev \n",
    "            count = 0\n",
    "            for pc_h, gpc_orig, ps_h, ep_h, m, s_c_p, ims in time_frame:\n",
    "            \n",
    "                gpc_h = copy.deepcopy(gpc_orig)\n",
    "                #generate target rep\n",
    "                shifted_ep_h = np.expand_dims(np.array(ep_h['translation']),1) - np.expand_dims(needed_ego_pose['translation'], 1)\n",
    "                transformed_ep_h = np.dot(rotation_matrix, shifted_ep_h[:2,:])\n",
    "                x = math.floor((int(transformed_ep_h[0]) + offset)/res)\n",
    "                y = math.floor((int(transformed_ep_h[1]) + offset)/res)\n",
    "                target_rep = np.zeros((256,256))\n",
    "                target_rep[y][x] += 1\n",
    "                \n",
    "                print(y,x)\n",
    "                \n",
    "#                 import pdb; pdb.set_trace()\n",
    "\n",
    "                #generate all reps except target\n",
    "\n",
    "                gpc_h.points[:3,:] -= np.expand_dims(needed_ego_pose['translation'], 1)\n",
    "                gpc_h.points[:2,:] = np.dot(rotation_matrix, gpc_h.points[:2,:])\n",
    "                \n",
    "#                 import pdb; pdb.set_trace()\n",
    "                \n",
    "                min_coor_x = max(-offset + 1, np.min(gpc_h.points[0,:]))\n",
    "                min_coor_y = max(-offset + 1, np.min(gpc_h.points[1,:]))\n",
    "                \n",
    "                max_coor_x = min(offset - 1, np.max(gpc_h.points[0,:]))\n",
    "                max_coor_y = min(offset - 1, np.max(gpc_h.points[1,:]))\n",
    "                \n",
    "                print(min_coor_x, min_coor_y, max_coor_x, max_coor_y)\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                #ensures we don't get lidar points that are too far away\n",
    "                point_filter_1_x = gpc_h.points[0,:] > min_coor_x\n",
    "                point_filter_1_y = gpc_h.points[1,:] > min_coor_y\n",
    "                point_filter_1 = np.logical_and(point_filter_1_x, point_filter_1_y)\n",
    "\n",
    "                point_filter_2_x = gpc_h.points[0,:] < max_coor_x\n",
    "                point_filter_2_y = gpc_h.points[1,:] < max_coor_y\n",
    "                point_filter_2 = np.logical_and(point_filter_2_x, point_filter_2_y)\n",
    "\n",
    "                point_filter = np.logical_and(point_filter_1, point_filter_2)\n",
    "                \n",
    "#                 import pdb; pdb.set_trace()\n",
    "                m_f, m_f_l, m_f_r, m_b = m[0], m[1], m[2], m[3]\n",
    "                s_c_p_f, s_c_p_f_l, s_c_p_f_r, s_c_p_b = s_c_p[0], s_c_p[1], s_c_p[2], s_c_p[3]\n",
    "#                 overall_mask  = np.logical_or(m_f, m_f_l)\n",
    "#                 overall_mask  = np.logical_or(overall_mask, m_f_r)            \n",
    "#                 overall_mask  = np.logical_or(overall_mask, m_b)\n",
    "\n",
    "\n",
    "                all_colored = np.array([-1] * gpc_h.points.shape[1])\n",
    "                all_colored[m_f] = s_c_p[0]\n",
    "                all_colored[m_f_l] =  s_c_p[1]\n",
    "                all_colored[m_f_r] = s_c_p[2]\n",
    "                all_colored[m_b] = s_c_p[3]\n",
    "\n",
    "                m_f_l, m_f_r = np.logical_and(point_filter, m_f_l),np.logical_and(point_filter, m_f_r)\n",
    "                m_f, m_b = np.logical_and(point_filter, m_f),np.logical_and(point_filter, m_b)\n",
    "\n",
    "                s_c_p_f = all_colored[m_f]\n",
    "                s_c_p_f_l = all_colored[m_f_l]\n",
    "                s_c_p_f_r = all_colored[m_f_r]\n",
    "                s_c_p_b = all_colored[m_b]\n",
    "                \n",
    "#                 print_stats(s_c_p)\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 import pdb; pdb.set_trace()\n",
    "                road_rep_fl, lane_rep_fl, obstacle_rep_fl, vehicle_rep_fl = get_intermediate_rep(gpc_h.points, m_f_l, s_c_p_f_l)\n",
    "                road_rep_fr, lane_rep_fr, obstacle_rep_fr, vehicle_rep_fr = get_intermediate_rep(gpc_h.points, m_f_r, s_c_p_f_r)\n",
    "\n",
    "                road_rep_f, lane_rep_f, obstacle_rep_f, vehicle_rep_f = get_intermediate_rep(gpc_h.points, m_f, s_c_p_f)\n",
    "                road_rep_b, lane_rep_b, obstacle_rep_b, vehicle_rep_b = get_intermediate_rep(gpc_h.points, m_b, s_c_p_b)\n",
    "\n",
    "                road_rep = road_rep_fl + road_rep_fr + road_rep_f + road_rep_b\n",
    "                lane_rep = lane_rep_fl + lane_rep_fr + lane_rep_f + lane_rep_b\n",
    "\n",
    "                obstacle_rep = obstacle_rep_fl + obstacle_rep_fr + obstacle_rep_f + obstacle_rep_b\n",
    "                vehicle_rep = vehicle_rep_fl + vehicle_rep_fr + vehicle_rep_f + vehicle_rep_b\n",
    "                print(np.count_nonzero(road_rep), np.count_nonzero(lane_rep),  np.count_nonzero(obstacle_rep), np.count_nonzero(vehicle_rep), np.count_nonzero(target_rep))\n",
    "                \n",
    "                if count == 3: \n",
    "                    import pdb; pdb.set_trace()\n",
    "                base_file_path = \"./representations\"\n",
    "                #plot the dilated versions of the intermediate reps \n",
    "                \n",
    "                kernel = np.ones((5,5), np.uint8)\n",
    "                dilation = cv2.dilate(road_rep,kernel,iterations = 1)\n",
    "                plt.imshow(dilation)\n",
    "                plt.savefig(base_file_path + \"/dilated_reps/road/road_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                \n",
    "                kernel = np.ones((2,2), np.uint8)\n",
    "                dilation = cv2.dilate(lane_rep,kernel,iterations = 1)\n",
    "                plt.imshow(dilation)\n",
    "                plt.savefig(base_file_path + \"/dilated_reps/lane/lane_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                \n",
    "                kernel = np.ones((5,5), np.uint8)\n",
    "                dilation = cv2.dilate(obstacle_rep,kernel,iterations = 1)\n",
    "                plt.imshow(dilation)\n",
    "                plt.savefig(base_file_path + \"/dilated_reps/obstacle/obstacle_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                \n",
    "                kernel = np.ones((2,2), np.uint8)\n",
    "                dilation = cv2.dilate(vehicle_rep,kernel,iterations = 1)\n",
    "                plt.imshow(dilation)\n",
    "                plt.savefig(base_file_path + \"/dilated_reps/vehicles/vehicle_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                \n",
    "                blur = cv2.GaussianBlur(copy.deepcopy(target_rep),(5,5),0)\n",
    "                \n",
    "                plt.imshow(blur)\n",
    "                plt.savefig(base_file_path + \"/dilated_reps/target/target_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                                \n",
    "                #plot current versions of the intermediate reps \n",
    "                plt.imshow(road_rep)\n",
    "                plt.savefig(base_file_path + \"/non_dilated_reps/road/road_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                \n",
    "                plt.imshow(lane_rep)\n",
    "                plt.savefig(base_file_path + \"/non_dilated_reps/lane/lane_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                \n",
    "                plt.imshow(obstacle_rep)\n",
    "                plt.savefig(base_file_path +\"/non_dilated_reps/obstacle/obstacle_{}.png\".format(count))\n",
    "                plt.clf()\n",
    "                assert(np.sum(target_rep) == 1)\n",
    "                      \n",
    "                plt.imshow(target_rep)\n",
    "                plt.savefig(base_file_path + \"/non_dilated_reps/target/target_{}.png\".format(count))\n",
    "                plt.clf()                \n",
    "                \n",
    "                            \n",
    "                plt.imshow(vehicle_rep)\n",
    "                plt.savefig(base_file_path + \"/non_dilated_reps/vehicles/vehicle_{}.png\".format(count))\n",
    "                plt.clf()                \n",
    "                \n",
    "                intermediate_reps.append([road_rep, lane_rep, obstacle_rep, vehicle_rep, target_rep])\n",
    "                count+=1 \n",
    "            \n",
    "            for k,data in enumerate(intermediate_reps):\n",
    "                if k < len(intermediate_reps) - 1:\n",
    "                    data.append(intermediate_reps[k+1][4])\n",
    "#             import pdb; pdb.set_trace()\n",
    "        \n",
    "        if not sample_rec['next'] == \"\":\n",
    "            current_token = sample_rec['next']\n",
    "        else:\n",
    "            has_more = False\n",
    "            \n",
    "        if len(current_seq) >= 12:\n",
    "            import pdb; pdb.set_trace()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_rep(BEV_points, mask, semantic_class_points):\n",
    "    corresponding_BEV_points= BEV_points[:,mask] \n",
    "    \n",
    "    num_points = corresponding_BEV_points.shape[1]\n",
    "    lane_rep =  np.zeros((256,256))\n",
    "    road_rep = np.zeros((256,256))\n",
    "    obstacle_rep = np.zeros((256,256))\n",
    "    vehicle_rep = np.zeros((256,256))\n",
    "    res = 0.5\n",
    "    #defining 128x128 region surrounding origin \n",
    "    offset = 64\n",
    "    for i in range(num_points):\n",
    "        current_point = np.round(corresponding_BEV_points[:,i][:2])\n",
    "        x = math.floor((int(current_point[0]) + offset)/res)\n",
    "        y = math.floor((int(current_point[1]) + offset)/res)\n",
    "        if x < 0 or y < 0 or x >= 256 or y >= 256: \n",
    "            import pdb; pdb.set_trace()\n",
    "        semantic_class = semantic_class_points[i]\n",
    "        if semantic_class == class_to_idx['Road']:\n",
    "            road_rep[y][x] += 1\n",
    "            \n",
    "        if semantic_class == class_to_idx['Lane Marking - General'] or semantic_class == class_to_idx['Lane Marking - Crosswalk']:\n",
    "            lane_rep[y][x] += 1\n",
    "            \n",
    "        if semantic_class == class_to_idx['Building'] or semantic_class == class_to_idx['Curb'] or semantic_class == class_to_idx['Vegetation']:\n",
    "            obstacle_rep[y][x] += 1\n",
    "        \n",
    "        if semantic_class == class_to_idx['Car'] or semantic_class == class_to_idx['Truck']:\n",
    "            vehicle_rep[y][x] += 1\n",
    "    \n",
    "    return road_rep, lane_rep, obstacle_rep, vehicle_rep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "### NEW STRUCTURE\n",
    "#for every scene (85)\n",
    "    #for every sequence (start of history to end of future ground truth) around 28 or 29\n",
    "        #store intermediate reps with ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#talk to Nachiket about ground truths\n",
    "#create data directory above\n",
    "#work on training script \n",
    "#work on validation script - mimic what authors did"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
